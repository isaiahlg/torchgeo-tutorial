{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html) \\|\\| \n",
    "[**Quickstart**](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) \\|\\|\n",
    "[Tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) \\|\\| [Datasets &\n",
    "DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) \\|\\|\n",
    "[Transforms](https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html) \\|\\| [Build\n",
    "Model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) \\|\\|\n",
    "[Autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) \\|\\|\n",
    "[Optimization](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) \\|\\| [Save & Load\n",
    "Model](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html)\n",
    "\n",
    "Pytorch Quickstart\n",
    "==========\n",
    "\n",
    "This section runs through the API for common tasks in machine learning.\n",
    "Refer to the links in each section to dive deeper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Environment Setup\n",
    "\n",
    "You need a Python environment to run this notebook. You can either do this in Google Colab, or by setting up a local python environment. I like to set my up using conda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a conda environment with python 3.11 for pytorch\n",
    "%conda create --name pyt python=3.11\n",
    "%conda activate pyt\n",
    "\n",
    "# run this next line specific to your OS and GPU configuration. Use CPU if you don't have CUDA.\n",
    "# https://pytorch.org/get-started/locally/\n",
    "# this example is for a linux system with CUDA 12.1\n",
    "%conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Working with data\n",
    "-----------------\n",
    "\n",
    "PyTorch has two [primitives to work with\n",
    "data](https://pytorch.org/docs/stable/data.html):\n",
    "`torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset`\n",
    "stores the samples and their corresponding labels, and `DataLoader`\n",
    "wraps an iterable around the `Dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries such as\n",
    "[TorchText](https://pytorch.org/text/stable/index.html),\n",
    "[TorchVision](https://pytorch.org/vision/stable/index.html), and\n",
    "[TorchAudio](https://pytorch.org/audio/stable/index.html), all of which\n",
    "include datasets. For this tutorial, we will be using a TorchVision\n",
    "dataset.\n",
    "\n",
    "The `torchvision.datasets` module contains `Dataset` objects for many\n",
    "real-world vision data like CIFAR, COCO ([full list\n",
    "here](https://pytorch.org/vision/stable/datasets.html)). In this\n",
    "tutorial, we use the FashionMNIST dataset. Every TorchVision `Dataset`\n",
    "includes two arguments: `transform` and `target_transform` to modify the\n",
    "samples and labels respectively.\n",
    "\n",
    "[Fashion-MNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist) is a dataset comprising of 28Ã—28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST shares the same image size, data format and the structure of training and testing splits with the original MNIST.\n",
    "\n",
    "![fashion-mnist](figures/fashion-mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the `Dataset` as an argument to `DataLoader`. This wraps an\n",
    "iterable over our dataset, and supports automatic batching, sampling,\n",
    "shuffling and multiprocess data loading. Here we define a batch size of\n",
    "64, i.e. each element in the dataloader iterable will return a batch of\n",
    "64 features and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [loading data in PyTorch](data_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Models\n",
    "===============\n",
    "\n",
    "To define a neural network in PyTorch, we create a class that inherits\n",
    "from\n",
    "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "We define the layers of the network in the `__init__` function and\n",
    "specify how data will pass through the network in the `forward`\n",
    "function. To accelerate operations in the neural network, we move it to\n",
    "the GPU or MPS if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [building neural networks in\n",
    "PyTorch](buildmodel_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the Model Parameters\n",
    "===============================\n",
    "\n",
    "To train a model, we need a [loss\n",
    "function](https://pytorch.org/docs/stable/nn.html#loss-functions) and an\n",
    "[optimizer](https://pytorch.org/docs/stable/optim.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training\n",
    "dataset (fed to it in batches), and backpropagates the prediction error\n",
    "to adjust the model\\'s parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check the model\\'s performance against the test dataset to\n",
    "ensure it is learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (*epochs*).\n",
    "During each epoch, the model learns parameters to make better\n",
    "predictions. We print the model\\'s accuracy and loss at each epoch;\n",
    "we\\'d like to see the accuracy increase and the loss decrease with every\n",
    "epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.297413  [   64/60000]\n",
      "loss: 2.283615  [ 6464/60000]\n",
      "loss: 2.261835  [12864/60000]\n",
      "loss: 2.258140  [19264/60000]\n",
      "loss: 2.246580  [25664/60000]\n",
      "loss: 2.210469  [32064/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m      5\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m      5\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilyon\\.conda\\envs\\pyt\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ilyon\\.conda\\envs\\pyt\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ilyon\\.conda\\envs\\pyt\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ilyon\\.conda\\envs\\pyt\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ilyon\\.conda\\envs\\pyt\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\ilyon\\.conda\\envs\\pyt\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\ilyon\\.conda\\envs\\pyt\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [Training your model](optimization_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "=============\n",
    "\n",
    "A common way to save a model is to serialize the internal state\n",
    "dictionary (containing the model parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to models/FashionMNIST/model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"models/FashionMNIST/model.pth\")\n",
    "print(\"Saved PyTorch Model State to models/FashionMNIST/model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Models\n",
    "==============\n",
    "\n",
    "The process for loading a model includes re-creating the model structure\n",
    "and loading the state dictionary into it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"models/FashionMNIST/model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can now be used to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdUElEQVR4nO3db2yV9f3/8ddpKYd/7altaU+P/LH8EYxAl6F0HcpUGkq3GBFuqPMGGqLBFTNk6sIyQbdlnSxxxoXpbiwwM1FnMmCaSILVlmwrGFBCzEZDSZUibZlozymtbbH9/G7ws98d+fu5OO27Lc9H8knoOde717tXr/bFOefq+4Scc04AAAyyNOsGAABXJwIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJkZZN/BNfX19OnHihDIzMxUKhazbAQB4cs6pvb1dsVhMaWkXfpwz5ALoxIkTmjx5snUbAIAr1NTUpEmTJl3w/iH3FFxmZqZ1CwCAFLjU7/MBC6DNmzfruuuu05gxY1RSUqL333//sup42g0ARoZL/T4fkAB6/fXXtW7dOm3cuFEffPCBiouLVV5erpMnTw7E7gAAw5EbAAsWLHCVlZX9H/f29rpYLOaqqqouWRuPx50kFovFYg3zFY/HL/r7PuWPgHp6enTgwAGVlZX135aWlqaysjLV1dWds313d7cSiUTSAgCMfCkPoM8++0y9vb0qKChIur2goEAtLS3nbF9VVaVIJNK/uAIOAK4O5lfBrV+/XvF4vH81NTVZtwQAGAQp/zugvLw8paenq7W1Nen21tZWRaPRc7YPh8MKh8OpbgMAMMSl/BHQ6NGjNX/+fFVXV/ff1tfXp+rqapWWlqZ6dwCAYWpAJiGsW7dOK1eu1E033aQFCxbo+eefV0dHhx588MGB2B0AYBgakAC655579N///lcbNmxQS0uLvvWtb2nXrl3nXJgAALh6hZxzzrqJ/5VIJBSJRKzbAABcoXg8rqysrAveb34VHADg6kQAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATKQ+gp59+WqFQKGnNnj071bsBAAxzowbik95444165513/m8nowZkNwCAYWxAkmHUqFGKRqMD8akBACPEgLwGdOTIEcViMU2bNk3333+/jh07dsFtu7u7lUgkkhYAYORLeQCVlJRo69at2rVrl1588UU1Njbq1ltvVXt7+3m3r6qqUiQS6V+TJ09OdUsAgCEo5JxzA7mDtrY2TZ06Vc8995xWrVp1zv3d3d3q7u7u/ziRSBBCADACxONxZWVlXfD+Ab86IDs7W9dff70aGhrOe384HFY4HB7oNgAAQ8yA/x3Q6dOndfToURUWFg70rgAAw0jKA+jxxx9XbW2tPv74Y/3rX//S3XffrfT0dN13332p3hUAYBhL+VNwx48f13333adTp05p4sSJuuWWW7R3715NnDgx1bsCAAxjA34Rgq9EIqFIJGLdBgDgCl3qIgRmwQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADAx4G9IBwAXkp6e7l3T19fnXTOYM5eDvMHm/74r9OWaMWOGd42kC745qAUeAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATDANG7hCoVBoUGqCTIG+9tprvWskqbS01Lvm7bff9q7p6Ojwrhnqgky2DmLFihWB6p599tkUdxIcj4AAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYYBgpYCDIYNEgbr311kB1JSUl3jWxWMy75oUXXvCuGery8/O9a8rLy71rEomEd81QwyMgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJhhGClyh9PR075qvvvrKu+amm27yrrnhhhu8aySptbXVu2bmzJneNdu3b/eu+fzzz71rxo4d610jSZ988ol3TW5urndNVlaWd83x48e9a4YaHgEBAEwQQAAAE94BtGfPHt15552KxWIKhULasWNH0v3OOW3YsEGFhYUaO3asysrKdOTIkVT1CwAYIbwDqKOjQ8XFxdq8efN579+0aZNeeOEFvfTSS9q3b5/Gjx+v8vJydXV1XXGzAICRw/sihIqKClVUVJz3Puecnn/+ef385z/XXXfdJUl6+eWXVVBQoB07dujee++9sm4BACNGSl8DamxsVEtLi8rKyvpvi0QiKikpUV1d3Xlruru7lUgkkhYAYORLaQC1tLRIkgoKCpJuLygo6L/vm6qqqhSJRPrX5MmTU9kSAGCIMr8Kbv369YrH4/2rqanJuiUAwCBIaQBFo1FJ5/4RW2tra/993xQOh5WVlZW0AAAjX0oDqKioSNFoVNXV1f23JRIJ7du3T6WlpancFQBgmPO+Cu706dNqaGjo/7ixsVEHDx5UTk6OpkyZorVr1+pXv/qVZs6cqaKiIj311FOKxWJatmxZKvsGAAxz3gG0f/9+3X777f0fr1u3TpK0cuVKbd26VU8++aQ6Ojr08MMPq62tTbfccot27dqlMWPGpK5rAMCwF3LOOesm/lcikVAkErFuA1eptDT/Z6X7+vq8a8aPH+9ds2HDBu+a7u5u7xop2Nd03XXXeddkZ2d713zxxRfeNUH/Axzk+xTkQqog513Q7+3atWsD1QURj8cv+rq++VVwAICrEwEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADAhPfbMWBoC4VC3jVBB6IHmeAbZF9BatLT071rJKm3tzdQna/Vq1d717S0tHjXdHV1eddIwSZbB5k4/c13T74cQb63QaZ7S1JHR4d3TU9Pj3dNkHeCDofD3jVSsAnfQY7D5eAREADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMMIx0kgzUkNOhg0SCCDnj0FWT45GANFZWk++67z7smGo1613zwwQfeNRkZGd41kpSdne1dc+rUKe+azz//3LsmLy/PuyYzM9O7Rgo+1NZXkMG+48aNC7SvmTNnetccPHgw0L4uhUdAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATDCMdJAM1pDQIEMNg9RIwQZ+BjkOgzlY9MEHH/SumTVrlndNU1OTd02QIZxBhuBK0tixY71rPv30U++aIENCgwzB7ezs9K6RpDFjxnjXDNbg4aDKy8u9axhGCgAYUQggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJi4qoeRBh3CGUSQYYNBhhoGGdQYpGYwxWIx75rly5cH2leQIZxHjhzxrpkwYYJ3TTgc9q7Jzc31rpGknp4e75og5/i4ceO8a4IIOtC2u7t7UPbV0dHhXRP053bhwoWB6gYCj4AAACYIIACACe8A2rNnj+68807FYjGFQiHt2LEj6f4HHnhAoVAoaS1dujRV/QIARgjvAOro6FBxcbE2b958wW2WLl2q5ubm/vXqq69eUZMAgJHH+yKEiooKVVRUXHSbcDisaDQauCkAwMg3IK8B1dTUKD8/X7NmzdIjjzyiU6dOXXDb7u5uJRKJpAUAGPlSHkBLly7Vyy+/rOrqaj377LOqra1VRUXFBS9NrKqqUiQS6V+TJ09OdUsAgCEo5X8HdO+99/b/e+7cuZo3b56mT5+umpoaLV68+Jzt169fr3Xr1vV/nEgkCCEAuAoM+GXY06ZNU15enhoaGs57fzgcVlZWVtICAIx8Ax5Ax48f16lTp1RYWDjQuwIADCPeT8GdPn066dFMY2OjDh48qJycHOXk5OiZZ57RihUrFI1GdfToUT355JOaMWOGysvLU9o4AGB48w6g/fv36/bbb+//+OvXb1auXKkXX3xRhw4d0p///Ge1tbUpFotpyZIl+uUvfxlojhUAYOQKuSATBAdQIpFQJBJRWlqa1zDOoMMGIU2cODFQ3dSpU71rZs+e7V0T5OnbIMM0Jamrq8u7Jshg0SCvdWZkZHjXBBmuKknjx48flJogX1NbW5t3TdDfD+np6d41QQaLnjlzxrsmyHknSZFIxLvm17/+tdf2vb29Onz4sOLx+EXPdWbBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMpPwtuVOlr69vwPdRUFAQqC7IFOjBmi4cZPpxUVGRd40kjRs3zrsmyNTf06dPe9ekpQX7v1WQScFBjvlXX33lXRPkeHd2dnrXSFJ3d7d3zejRo71rmpubvWuCfI+CHDtJ+uKLL7xrgkypvuaaa7xrgkzdlqRoNOpdk5ub67X95Z7fPAICAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgYsgOI/VVVlbmXROLxQLtK8hAzfz8fO+aIAM1gwxxDfL1SFJ7e7t3TZBBjUGGJ4ZCIe8aSQqHw941QQZWBvneBjl26enp3jVSsEGXQc6HeDzuXRPkZ2kwBTkfgvzcBhmCKwUbGus7PJdhpACAIY0AAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJITuM9I477tCoUZff3qpVq7z3cfjwYe8aSWpubvauSSQS3jVBBkn29PQMyn6CCjKwMsjwxN7eXu8aScrKyvKuCTL4NMggySADKzMyMrxrpGADYAsKCrxrbrzxRu+aIF/TYJ7jQQa5jhs3zrumq6vLu0YK1t/Jkye9tr/cc5VHQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwM2WGkBw4c8Bry+J3vfMd7H3PnzvWukaSFCxcGqvP11VdfedcEGfb5+eefe9cErYvH4941QYaRBhkQKkm5ubneNbNmzfKuCTJ8MsigVOecd40kFRcXe9ccOnTIu+bjjz/2rikrK/OuCYfD3jVS8OPnK8jP+qeffhpoX0EGI0+YMMFr+8sdBswjIACACQIIAGDCK4Cqqqp08803KzMzU/n5+Vq2bJnq6+uTtunq6lJlZaVyc3M1YcIErVixQq2trSltGgAw/HkFUG1trSorK7V3717t3r1bZ86c0ZIlS5Le4Oixxx7Tm2++qTfeeEO1tbU6ceKEli9fnvLGAQDDm9dFCLt27Ur6eOvWrcrPz9eBAwe0aNEixeNx/elPf9K2bdt0xx13SJK2bNmiG264QXv37g10oQAAYGS6oteAvr6iKScnR9LZK9fOnDmTdJXK7NmzNWXKFNXV1Z33c3R3dyuRSCQtAMDIFziA+vr6tHbtWi1cuFBz5syRJLW0tGj06NHKzs5O2ragoEAtLS3n/TxVVVWKRCL9a/LkyUFbAgAMI4EDqLKyUh999JFee+21K2pg/fr1isfj/aupqemKPh8AYHgI9Ieoa9as0VtvvaU9e/Zo0qRJ/bdHo1H19PSora0t6VFQa2urotHoeT9XOBwO/EdiAIDhy+sRkHNOa9as0fbt2/Xuu++qqKgo6f758+crIyND1dXV/bfV19fr2LFjKi0tTU3HAIARwesRUGVlpbZt26adO3cqMzOz/3WdSCSisWPHKhKJaNWqVVq3bp1ycnKUlZWlRx99VKWlpVwBBwBI4hVAL774oiTptttuS7p9y5YteuCBByRJv/vd75SWlqYVK1aou7tb5eXl+sMf/pCSZgEAI0fIDda0vcuUSCQUiUSs27go38F8klRSUuJdc/3113vXfPe73/Wuyc/P966Rgg3HHD9+vHdNkMGiQU/rvr4+75ogQ1kPHz7sXbN7927vmrffftu7Rjo70WSo+vvf/+5dM2XKlED7+uyzz7xrggwEDlITZICpdPZPX3w9/vjjXts759TZ2al4PH7R3xPMggMAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmGAaNgBgQDANGwAwJBFAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEx4BVBVVZVuvvlmZWZmKj8/X8uWLVN9fX3SNrfddptCoVDSWr16dUqbBgAMf14BVFtbq8rKSu3du1e7d+/WmTNntGTJEnV0dCRt99BDD6m5ubl/bdq0KaVNAwCGv1E+G+/atSvp461btyo/P18HDhzQokWL+m8fN26cotFoajoEAIxIV/QaUDwelyTl5OQk3f7KK68oLy9Pc+bM0fr169XZ2XnBz9Hd3a1EIpG0AABXARdQb2+v+8EPfuAWLlyYdPsf//hHt2vXLnfo0CH3l7/8xV177bXu7rvvvuDn2bhxo5PEYrFYrBG24vH4RXMkcACtXr3aTZ061TU1NV10u+rqaifJNTQ0nPf+rq4uF4/H+1dTU5P5QWOxWCzWla9LBZDXa0BfW7Nmjd566y3t2bNHkyZNuui2JSUlkqSGhgZNnz79nPvD4bDC4XCQNgAAw5hXADnn9Oijj2r79u2qqalRUVHRJWsOHjwoSSosLAzUIABgZPIKoMrKSm3btk07d+5UZmamWlpaJEmRSERjx47V0aNHtW3bNn3/+99Xbm6uDh06pMcee0yLFi3SvHnzBuQLAAAMUz6v++gCz/Nt2bLFOefcsWPH3KJFi1xOTo4Lh8NuxowZ7oknnrjk84D/Kx6Pmz9vyWKxWKwrX5f63R/6/8EyZCQSCUUiEes2AABXKB6PKysr64L3MwsOAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGBiyAWQc866BQBAClzq9/mQC6D29nbrFgAAKXCp3+chN8QecvT19enEiRPKzMxUKBRKui+RSGjy5MlqampSVlaWUYf2OA5ncRzO4jicxXE4aygcB+ec2tvbFYvFlJZ24cc5owaxp8uSlpamSZMmXXSbrKysq/oE+xrH4SyOw1kch7M4DmdZH4dIJHLJbYbcU3AAgKsDAQQAMDGsAigcDmvjxo0Kh8PWrZjiOJzFcTiL43AWx+Gs4XQchtxFCACAq8OwegQEABg5CCAAgAkCCABgggACAJgYNgG0efNmXXfddRozZoxKSkr0/vvvW7c06J5++mmFQqGkNXv2bOu2BtyePXt05513KhaLKRQKaceOHUn3O+e0YcMGFRYWauzYsSorK9ORI0dsmh1AlzoODzzwwDnnx9KlS22aHSBVVVW6+eablZmZqfz8fC1btkz19fVJ23R1damyslK5ubmaMGGCVqxYodbWVqOOB8blHIfbbrvtnPNh9erVRh2f37AIoNdff13r1q3Txo0b9cEHH6i4uFjl5eU6efKkdWuD7sYbb1Rzc3P/+sc//mHd0oDr6OhQcXGxNm/efN77N23apBdeeEEvvfSS9u3bp/Hjx6u8vFxdXV2D3OnAutRxkKSlS5cmnR+vvvrqIHY48Gpra1VZWam9e/dq9+7dOnPmjJYsWaKOjo7+bR577DG9+eabeuONN1RbW6sTJ05o+fLlhl2n3uUcB0l66KGHks6HTZs2GXV8AW4YWLBggausrOz/uLe318ViMVdVVWXY1eDbuHGjKy4utm7DlCS3ffv2/o/7+vpcNBp1v/3tb/tva2trc+Fw2L366qsGHQ6Obx4H55xbuXKlu+uuu0z6sXLy5EknydXW1jrnzn7vMzIy3BtvvNG/zX/+8x8nydXV1Vm1OeC+eRycc+573/ue+/GPf2zX1GUY8o+Aenp6dODAAZWVlfXflpaWprKyMtXV1Rl2ZuPIkSOKxWKaNm2a7r//fh07dsy6JVONjY1qaWlJOj8ikYhKSkquyvOjpqZG+fn5mjVrlh555BGdOnXKuqUBFY/HJUk5OTmSpAMHDujMmTNJ58Ps2bM1ZcqUEX0+fPM4fO2VV15RXl6e5syZo/Xr16uzs9OivQsacsNIv+mzzz5Tb2+vCgoKkm4vKCjQ4cOHjbqyUVJSoq1bt2rWrFlqbm7WM888o1tvvVUfffSRMjMzrdsz0dLSIknnPT++vu9qsXTpUi1fvlxFRUU6evSofvazn6miokJ1dXVKT0+3bi/l+vr6tHbtWi1cuFBz5syRdPZ8GD16tLKzs5O2Hcnnw/mOgyT98Ic/1NSpUxWLxXTo0CH99Kc/VX19vf72t78ZdptsyAcQ/k9FRUX/v+fNm6eSkhJNnTpVf/3rX7Vq1SrDzjAU3Hvvvf3/njt3rubNm6fp06erpqZGixcvNuxsYFRWVuqjjz66Kl4HvZgLHYeHH364/99z585VYWGhFi9erKNHj2r69OmD3eZ5Dfmn4PLy8pSenn7OVSytra2KRqNGXQ0N2dnZuv7669XQ0GDdipmvzwHOj3NNmzZNeXl5I/L8WLNmjd566y299957SW/fEo1G1dPTo7a2tqTtR+r5cKHjcD4lJSWSNKTOhyEfQKNHj9b8+fNVXV3df1tfX5+qq6tVWlpq2Jm906dP6+jRoyosLLRuxUxRUZGi0WjS+ZFIJLRv376r/vw4fvy4Tp06NaLOD+ec1qxZo+3bt+vdd99VUVFR0v3z589XRkZG0vlQX1+vY8eOjajz4VLH4XwOHjwoSUPrfLC+CuJyvPbaay4cDrutW7e6f//73+7hhx922dnZrqWlxbq1QfWTn/zE1dTUuMbGRvfPf/7TlZWVuby8PHfy5Enr1gZUe3u7+/DDD92HH37oJLnnnnvOffjhh+6TTz5xzjn3m9/8xmVnZ7udO3e6Q4cOubvuussVFRW5L7/80rjz1LrYcWhvb3ePP/64q6urc42Nje6dd95x3/72t93MmTNdV1eXdesp88gjj7hIJOJqampcc3Nz/+rs7OzfZvXq1W7KlCnu3Xffdfv373elpaWutLTUsOvUu9RxaGhocL/4xS/c/v37XWNjo9u5c6ebNm2aW7RokXHnyYZFADnn3O9//3s3ZcoUN3r0aLdgwQK3d+9e65YG3T333OMKCwvd6NGj3bXXXuvuuece19DQYN3WgHvvvfecpHPWypUrnXNnL8V+6qmnXEFBgQuHw27x4sWuvr7etukBcLHj0NnZ6ZYsWeImTpzoMjIy3NSpU91DDz004v6Tdr6vX5LbsmVL/zZffvml+9GPfuSuueYaN27cOHf33Xe75uZmu6YHwKWOw7Fjx9yiRYtcTk6OC4fDbsaMGe6JJ55w8XjctvFv4O0YAAAmhvxrQACAkYkAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJ/wfS3ncBjBZLmwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x.shape)\n",
    "\n",
    "# visualize the tensor as an image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x[0], cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [Saving & Loading your\n",
    "model](saveloadrun_tutorial.html).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
