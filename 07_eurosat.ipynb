{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoML with EuroSAT Notebook\n",
    "In this notebook, we download and visualize EuroSAT data from both the spatial and non-spatial splits. We then train two ResNet50 image encoders on each dataset, one with randomly initialized weights and one with pre-initialized weights from SSL4EO-S12. We then evaluate each on the test and train data, and then customize the training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isly9493/miniconda3/envs/pyt2/lib/python3.11/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/home/isly9493/miniconda3/envs/pyt2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# data libraries\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgeo.datasets import EuroSAT, EuroSATSpatial\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ml libraries\n",
    "import torch # for model training\n",
    "from torch import nn # for neural network layers\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torchgeo.models import resnet50 # import resnet50 model from torchvision\n",
    "from torchgeo.models import ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and Visualize EuroSat Data\n",
    "\n",
    "In this section, we use the torchgeo dataset/datamodules to download the EuroSat dataset (spatial and nonspatial split versions). We then plot a random sample of 6 images (with labels) from the training set and the test set for each dataset version. We also calculate the number of samples in each split for each version (spatial and nonspatial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EuroSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "Number of images in train dataset: 16200\n",
      "Number of images in test dataset: 5400\n"
     ]
    }
   ],
   "source": [
    "eurosat_root = os.path.join(\"data\", \"eurosat\")\n",
    "eurosat_dataset_train = EuroSAT(eurosat_root, split=\"train\", download=True)\n",
    "eurosat_dataset_test = EuroSAT(eurosat_root, split=\"test\", download=True)\n",
    "\n",
    "print(f'Dataset Classes: {eurosat_dataset_train.classes}')\n",
    "print(f'Number of images in train dataset: {len(eurosat_dataset_train)}')\n",
    "print(f'Number of images in test dataset: {len(eurosat_dataset_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EuroSAT Spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "Number of images in train dataset: 16200\n",
      "Number of images in test dataset: 5400\n"
     ]
    }
   ],
   "source": [
    "eurosat_spatial_root = os.path.join(\"data\", \"eurosatSpatial\")\n",
    "eurosat_spatial_dataset_train = EuroSATSpatial(eurosat_spatial_root, split=\"train\", download=True)\n",
    "eurosat_spatial_dataset_test = EuroSATSpatial(eurosat_spatial_root, split=\"test\", download=True)\n",
    "\n",
    "print(f'Dataset Classes: {eurosat_spatial_dataset_train.classes}')\n",
    "print(f'Number of images in train dataset: {len(eurosat_spatial_dataset_train)}')\n",
    "print(f'Number of images in test dataset: {len(eurosat_spatial_dataset_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # of Images in Each Dataset Split\n",
    "Split    | %            | EuroSAT   | EuroSAT Spatial   |\n",
    "---------|-----------   |---------  |---------          |\n",
    "Train    |  60          |   16200   |   16200           |\n",
    "Validate |  20          |   5400    |   5400            |\n",
    "Test     |  20          |   5400    |   5400            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Images and Labels from Each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "random.seed(44)\n",
    "\n",
    "eurosat_datasets = {\n",
    "    \"eurosat_train\": eurosat_dataset_train,\n",
    "    \"eurosat_test\": eurosat_dataset_test,\n",
    "    \"eurosat_spatial_train\": eurosat_spatial_dataset_train,\n",
    "    \"eurosat_spatial_test\": eurosat_spatial_dataset_test\n",
    "}\n",
    "figures_base_path = os.path.join(\"figures\", \"eurosat100\")\n",
    "\n",
    "\n",
    "# sample 6 random images, plot, and export the figures\n",
    "for name, dataset in eurosat_datasets.items():\n",
    "    n = 0\n",
    "    for i in random.sample(range(len(dataset)), 6):\n",
    "        fig = dataset.plot(\n",
    "            sample=dataset.__getitem__(i),\n",
    "            show_titles=True\n",
    "        )\n",
    "        # export fig to png\n",
    "        path = os.path.join(\"figures\", \"eurosat\", f\"{name}_{n}.png\")\n",
    "        fig.savefig(path)\n",
    "        plt.close(fig)\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize just one image\n",
    "fig = eurosat_dataset_train.plot(\n",
    "    sample=eurosat_dataset_train.__getitem__(1),\n",
    "    show_titles=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train & Evaluate a Model on the EuroSAT Data \n",
    "\n",
    "In this section, we train two ResNet 50 models on the nonspatial EuroSat dataset: one with randomly initialized weights, and one with pre-initialized weights from the SSL4EO-S12 project (use the pretrained weights available in torchgeo). We report the performance of these models on the train and test sets in a table. In addition to the table, we report (i) how we implemented the training and evaluation procedure, what hyperparameters we use to train the model, and how we choose those hyperparameters (e.g. if we perform a cross-validation search for hyperparameters, we explain how you conducted that search), (ii) what pretrained weights we choose to use, and a short description of the process used to generate those weights, (iii) a short paragraph describing the results in your table in words.\n",
    "\n",
    "The flow of this section is as follows:\n",
    "1. Configure the GPU\n",
    "1. Define the Train and Test Loos\n",
    "1. Define a ResNet model with random weights, customize the model for EuroSat data\n",
    "1. Define a data sampler and loader\n",
    "1. Load and transform the data\n",
    "1. Train & evaluate the model\n",
    "1. Repeat with a pre-trained ResNet50\n",
    "1. Report results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure GPU\n",
    "\n",
    "Note: there are additional steps required on Mac to use your processor as a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "torch.manual_seed(44)\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Train & Test Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "# from the pytorch tutorial\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, sample in enumerate(dataloader):\n",
    "        X, y = sample['image'], sample['label']\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print intermediate results every n batches\n",
    "        n = 20\n",
    "        if batch % n == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loop\n",
    "# from the pytorch tutorial\n",
    "def test(dataloader, model, loss_fn, val=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for sample in dataloader:\n",
    "            X, y = sample['image'], sample['label']\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    prefix = \"Validation\" if val else \"Test\"\n",
    "    print(f\"{prefix} Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eurosat_data(txs, batch_size=64):\n",
    "    # todo: reimplement with a for loop over train, val, and test\n",
    "    # reload data with the new transform\n",
    "    root = os.path.join(\"data\", \"eurosat\")\n",
    "    dataset_train = EuroSAT(root, split=\"train\", download=True, transforms=txs)\n",
    "    dataset_val = EuroSAT(root, split=\"val\", download=True, transforms=txs)\n",
    "    dataset_test = EuroSAT(root, split=\"test\", download=True, transforms=txs)\n",
    "\n",
    "    # define a sampler for the EuroSAT dataset\n",
    "    # it's a non-spatial dataset, so we can use a regular sampler from pytorch\n",
    "    sampler_train = RandomSampler(dataset_train, replacement=False) # start with a small batch\n",
    "    sampler_val = RandomSampler(dataset_val, replacement=False) # start with a small batch\n",
    "    sampler_test = RandomSampler(dataset_test, replacement=False) # start with a small batch\n",
    "\n",
    "    # define a dataloader to iterate over the dataset\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, sampler=sampler_train)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=batch_size, sampler=sampler_val)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, sampler=sampler_test)\n",
    "    \n",
    "    return dataloader_train, dataloader_val, dataloader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transform function to handle the dictionary structure of torchgeo dataset\n",
    "class CustomTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample['image'] = self.transform(sample['image'])\n",
    "        return sample\n",
    "    \n",
    "# Define transformations for the dataset to get it from 64x64 to 224x224\n",
    "transform = transforms.Resize((224, 224))  # Resizes the images to 224x224\n",
    "\n",
    "custom_transform = CustomTransform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Train, Val, Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test(dataloader_train, dataloader_val, dataloader_test, model, loss_fn=nn.CrossEntropyLoss(), lr=1e-3, epochs=5):\n",
    "    # create optimizer from the model and the learning rate\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    # baseline before training\n",
    "    print(f\"Baseline Before Training\\n-------------------------------\")\n",
    "    test(dataloader_test, model, loss_fn, val=False)\n",
    "    test(dataloader_val, model, loss_fn, val=True)\n",
    "\n",
    "    # run train / val loop\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(dataloader_train, model, loss_fn, optimizer)\n",
    "        test(dataloader_val, model, loss_fn, val=True)\n",
    "\n",
    "    # test result with test data\n",
    "    print(\"Test Results\\n-------------------------------\")\n",
    "    test(dataloader_test, model, loss_fn, val=False)\n",
    "\n",
    "    print(\"Training Complete!\")\n",
    "\n",
    "def evaluate_model(model, dataloader_test, loss_fn=nn.CrossEntropyLoss()):\n",
    "    print(f\"Evaluating Model on Test Data\\n-------------------------------\")\n",
    "    test(dataloader_test, model, loss_fn, val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Function to Save Down the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, experiment_name):\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.makedirs(\"models\")\n",
    "    model_weights_path = os.path.join(\"models\", \"eurosat\", experiment_name)\n",
    "    torch.save(model.state_dict(), model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=None).to(device)\n",
    "\n",
    "# modify the ResNet model to have 10 output classes\n",
    "model.fc = nn.Linear(2048, 10).to(device)\n",
    "\n",
    "# modify the model to expect 13 channels instead of 3\n",
    "model.conv1 = nn.Conv2d(13, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the EuroSAT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train, dataloader_val, dataloader_test = load_eurosat_data(custom_transform, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, model, lr=1e-3, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"eurosat_resnet50_epochs10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefine Model with Pre-Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.SENTINEL2_ALL_MOCO).to(device) # note the model with these weights is 90.1MB\n",
    "# this version already expects 13 input channels\n",
    "# modify the ResNet model to have 10 output classes\n",
    "model.fc = nn.Linear(2048, 10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload EuroSAT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train, dataloader_val, dataloader_test = load_eurosat_data(custom_transform, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain & Retest the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Before Training\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 2.090453 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 2.100065 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.079106  [   64/16200]\n",
      "loss: 2.088745  [ 1344/16200]\n",
      "loss: 2.077664  [ 2624/16200]\n",
      "loss: 2.083632  [ 3904/16200]\n",
      "loss: 2.108026  [ 5184/16200]\n",
      "loss: 2.062145  [ 6464/16200]\n",
      "loss: 2.069866  [ 7744/16200]\n",
      "loss: 2.087678  [ 9024/16200]\n",
      "loss: 2.027572  [10304/16200]\n",
      "loss: 2.021021  [11584/16200]\n",
      "loss: 2.081037  [12864/16200]\n",
      "loss: 2.003981  [14144/16200]\n",
      "loss: 2.100436  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.022634 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.991411  [   64/16200]\n",
      "loss: 2.106288  [ 1344/16200]\n",
      "loss: 2.026760  [ 2624/16200]\n",
      "loss: 2.028827  [ 3904/16200]\n",
      "loss: 2.086024  [ 5184/16200]\n",
      "loss: 2.029194  [ 6464/16200]\n",
      "loss: 1.992813  [ 7744/16200]\n",
      "loss: 1.962950  [ 9024/16200]\n",
      "loss: 1.977433  [10304/16200]\n",
      "loss: 1.982259  [11584/16200]\n",
      "loss: 2.080275  [12864/16200]\n",
      "loss: 1.970692  [14144/16200]\n",
      "loss: 1.962248  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 74.0%, Avg loss: 1.979897 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.047324  [   64/16200]\n",
      "loss: 2.048075  [ 1344/16200]\n",
      "loss: 1.999990  [ 2624/16200]\n",
      "loss: 2.027963  [ 3904/16200]\n",
      "loss: 1.968830  [ 5184/16200]\n",
      "loss: 1.975179  [ 6464/16200]\n",
      "loss: 1.997782  [ 7744/16200]\n",
      "loss: 1.896571  [ 9024/16200]\n",
      "loss: 2.036611  [10304/16200]\n",
      "loss: 2.042494  [11584/16200]\n",
      "loss: 1.985109  [12864/16200]\n",
      "loss: 1.916421  [14144/16200]\n",
      "loss: 2.013758  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 75.6%, Avg loss: 1.940127 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.855103  [   64/16200]\n",
      "loss: 1.893471  [ 1344/16200]\n",
      "loss: 2.012566  [ 2624/16200]\n",
      "loss: 1.976352  [ 3904/16200]\n",
      "loss: 1.888088  [ 5184/16200]\n",
      "loss: 1.975083  [ 6464/16200]\n",
      "loss: 1.944904  [ 7744/16200]\n",
      "loss: 1.857981  [ 9024/16200]\n",
      "loss: 1.913461  [10304/16200]\n",
      "loss: 1.912285  [11584/16200]\n",
      "loss: 1.896472  [12864/16200]\n",
      "loss: 1.809632  [14144/16200]\n",
      "loss: 1.865680  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 74.6%, Avg loss: 1.875622 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.886214  [   64/16200]\n",
      "loss: 1.983725  [ 1344/16200]\n",
      "loss: 1.927892  [ 2624/16200]\n",
      "loss: 1.766153  [ 3904/16200]\n",
      "loss: 1.788016  [ 5184/16200]\n",
      "loss: 1.783711  [ 6464/16200]\n",
      "loss: 1.805781  [ 7744/16200]\n",
      "loss: 1.870896  [ 9024/16200]\n",
      "loss: 1.841278  [10304/16200]\n",
      "loss: 1.807109  [11584/16200]\n",
      "loss: 1.866477  [12864/16200]\n",
      "loss: 1.801436  [14144/16200]\n",
      "loss: 1.768945  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 76.6%, Avg loss: 1.818201 \n",
      "\n",
      "Test Results\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 1.799511 \n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, model, lr=1e-3, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Down the Fine-Tuned Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"eurosat_resnet50_pretrained_epochs5_v2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train & Evaluate a Model on the EuroSAT Spatial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define EuroSAT Spatial Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eurosat_spatial_data(txs, batch_size=64):\n",
    "    # reload data with the new transform\n",
    "    root = os.path.join(\"data\", \"eurosatSpatial\")\n",
    "    dataset_train = EuroSATSpatial(root, split=\"train\", download=True, transforms=txs)\n",
    "    dataset_val = EuroSATSpatial(root, split=\"val\", download=True, transforms=txs)\n",
    "    dataset_test = EuroSATSpatial(root, split=\"test\", download=True, transforms=txs)\n",
    "\n",
    "    # define a sampler for the EuroSATSpatial dataset\n",
    "    # it's a non-spatial dataset, so we can use a regular sampler from pytorch\n",
    "    sampler_train = RandomSampler(dataset_train, replacement=False) # start with a small batch\n",
    "    sampler_val = RandomSampler(dataset_val, replacement=False) # start with a small batch\n",
    "    sampler_test = RandomSampler(dataset_test, replacement=False) # start with a small batch\n",
    "\n",
    "    # define a dataloader to iterate over the dataset\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, sampler=sampler_train)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=batch_size, sampler=sampler_val)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, sampler=sampler_test)\n",
    "\n",
    "    return dataloader_train, dataloader_val, dataloader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=None).to(device)\n",
    "\n",
    "# modify the ResNet model to have 10 output classes\n",
    "model.fc = nn.Linear(2048, 10).to(device)\n",
    "\n",
    "# modify the model to expect 13 channels instead of 3\n",
    "model.conv1 = nn.Conv2d(13, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the EuroSAT Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train, dataloader_val, dataloader_test = load_eurosat_spatial_data(custom_transform, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, model, lr=1e-3, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Down the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"eurosatspatial_resnet50_epochs10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefine the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.SENTINEL2_ALL_MOCO).to(device) # note the model with these weights is 90.1MB\n",
    "\n",
    "# modify the ResNet model to have 10 output classes\n",
    "model.fc = nn.Linear(2048, 10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload EuroSAT Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train, dataloader_val, dataloader_test = load_eurosat_spatial_data(custom_transform, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain & Retest the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, model, lr=1e-3, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Down the Fine-Tuned Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"eurosatspatial_resnet50_pretrained_epochs10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Evaluate a Given Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(experiment_name):\n",
    "    # create an empty res net with the right io dimensions\n",
    "    model = resnet50(weights=None, in_chans=13, num_classes=10).to(device)\n",
    "    # create the path to the model weights\n",
    "    model_weights_path = os.path.join(\"models\", \"eurosat\", experiment_name)\n",
    "    # load the model weights from the path\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1681832/3585238199.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model on Test Data\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 1.810232 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"eurosat_resnet50_pretrained_epochs10.pth\"\n",
    "model = load_model(experiment_name)\n",
    "_, _, dataloader_test = load_eurosat_data(batch_size=64)\n",
    "evaluate_model(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add in Data Augmentation Techniques\n",
    "This section adds in additonal data transforms prior to loading the data that flip, rotate, noise, etc the data prior to training to create more samples. We then rerun some of the top-performing experiments to see how the results change. \n",
    "\n",
    "#### Define a New Custom Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the dataset to get it from 64x64 to 224x224\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # Randomly flip the image vertically\n",
    "    transforms.Resize((224, 224))  # Resizes the images to 224x224\n",
    "])\n",
    "\n",
    "custom_transform = CustomTransform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redo Training for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2603663/2779468065.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Before Training\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.803481 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.823417 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.804927  [   64/16200]\n",
      "loss: 0.624438  [ 1344/16200]\n",
      "loss: 0.825412  [ 2624/16200]\n",
      "loss: 0.691295  [ 3904/16200]\n",
      "loss: 0.845601  [ 5184/16200]\n",
      "loss: 0.705668  [ 6464/16200]\n",
      "loss: 0.950877  [ 7744/16200]\n",
      "loss: 0.561642  [ 9024/16200]\n",
      "loss: 0.719170  [10304/16200]\n",
      "loss: 0.579537  [11584/16200]\n",
      "loss: 0.665559  [12864/16200]\n",
      "loss: 0.887553  [14144/16200]\n",
      "loss: 0.731226  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.779164 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.670644  [   64/16200]\n",
      "loss: 0.753823  [ 1344/16200]\n",
      "loss: 0.662568  [ 2624/16200]\n",
      "loss: 0.834877  [ 3904/16200]\n",
      "loss: 0.806893  [ 5184/16200]\n",
      "loss: 0.526802  [ 6464/16200]\n",
      "loss: 0.691623  [ 7744/16200]\n",
      "loss: 0.593660  [ 9024/16200]\n",
      "loss: 0.665421  [10304/16200]\n",
      "loss: 0.803039  [11584/16200]\n",
      "loss: 0.715808  [12864/16200]\n",
      "loss: 0.626272  [14144/16200]\n",
      "loss: 0.689552  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.844126 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.720954  [   64/16200]\n",
      "loss: 0.590447  [ 1344/16200]\n",
      "loss: 0.653797  [ 2624/16200]\n",
      "loss: 0.803228  [ 3904/16200]\n",
      "loss: 0.692828  [ 5184/16200]\n",
      "loss: 0.728601  [ 6464/16200]\n",
      "loss: 0.636084  [ 7744/16200]\n",
      "loss: 0.714802  [ 9024/16200]\n",
      "loss: 0.444675  [10304/16200]\n",
      "loss: 0.672873  [11584/16200]\n",
      "loss: 0.713557  [12864/16200]\n",
      "loss: 0.637321  [14144/16200]\n",
      "loss: 0.582789  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.793916 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.831745  [   64/16200]\n",
      "loss: 0.548437  [ 1344/16200]\n",
      "loss: 0.564864  [ 2624/16200]\n",
      "loss: 0.580330  [ 3904/16200]\n",
      "loss: 0.728331  [ 5184/16200]\n",
      "loss: 0.546961  [ 6464/16200]\n",
      "loss: 0.611717  [ 7744/16200]\n",
      "loss: 0.626402  [ 9024/16200]\n",
      "loss: 0.630113  [10304/16200]\n",
      "loss: 0.629094  [11584/16200]\n",
      "loss: 0.732090  [12864/16200]\n",
      "loss: 0.549847  [14144/16200]\n",
      "loss: 0.534731  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.316041 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.722426  [   64/16200]\n",
      "loss: 0.595418  [ 1344/16200]\n",
      "loss: 0.644250  [ 2624/16200]\n",
      "loss: 0.629306  [ 3904/16200]\n",
      "loss: 0.661627  [ 5184/16200]\n",
      "loss: 0.552683  [ 6464/16200]\n",
      "loss: 0.687904  [ 7744/16200]\n",
      "loss: 0.449938  [ 9024/16200]\n",
      "loss: 0.509565  [10304/16200]\n",
      "loss: 0.520463  [11584/16200]\n",
      "loss: 0.550133  [12864/16200]\n",
      "loss: 0.553546  [14144/16200]\n",
      "loss: 0.639265  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.874908 \n",
      "\n",
      "Test Results\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.872826 \n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# model = resnet50(weights=None, in_chans=13, num_classes=10).to(device)\n",
    "model = load_model(\"eurosat_resnet50_imgflips_epochs5.pth\")\n",
    "dataloader_train, dataloader_val, dataloader_test = load_eurosat_data(custom_transform, batch_size=64)\n",
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, model, lr=1e-3, epochs=5)\n",
    "save_model(model, \"eurosat_resnet50_imgflips_epochs10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2603663/2779468065.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Before Training\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 2.066019 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 80.2%, Avg loss: 2.076467 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.086790  [   64/16200]\n",
      "loss: 2.080106  [ 1344/16200]\n",
      "loss: 2.076344  [ 2624/16200]\n",
      "loss: 2.082392  [ 3904/16200]\n",
      "loss: 2.084616  [ 5184/16200]\n",
      "loss: 2.072159  [ 6464/16200]\n",
      "loss: 2.080302  [ 7744/16200]\n",
      "loss: 2.045496  [ 9024/16200]\n",
      "loss: 2.045351  [10304/16200]\n",
      "loss: 2.035016  [11584/16200]\n",
      "loss: 2.051425  [12864/16200]\n",
      "loss: 1.990753  [14144/16200]\n",
      "loss: 2.022725  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 80.4%, Avg loss: 2.025832 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.980469  [   64/16200]\n",
      "loss: 2.023612  [ 1344/16200]\n",
      "loss: 2.044255  [ 2624/16200]\n",
      "loss: 2.008262  [ 3904/16200]\n",
      "loss: 2.000562  [ 5184/16200]\n",
      "loss: 1.952807  [ 6464/16200]\n",
      "loss: 2.004670  [ 7744/16200]\n",
      "loss: 2.005918  [ 9024/16200]\n",
      "loss: 2.044146  [10304/16200]\n",
      "loss: 1.959069  [11584/16200]\n",
      "loss: 2.021442  [12864/16200]\n",
      "loss: 1.964187  [14144/16200]\n",
      "loss: 1.967936  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 80.5%, Avg loss: 1.958091 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.911485  [   64/16200]\n",
      "loss: 1.987399  [ 1344/16200]\n",
      "loss: 2.070498  [ 2624/16200]\n",
      "loss: 1.950443  [ 3904/16200]\n",
      "loss: 1.909814  [ 5184/16200]\n",
      "loss: 2.055257  [ 6464/16200]\n",
      "loss: 1.923242  [ 7744/16200]\n",
      "loss: 2.055898  [ 9024/16200]\n",
      "loss: 1.884781  [10304/16200]\n",
      "loss: 1.903773  [11584/16200]\n",
      "loss: 1.858349  [12864/16200]\n",
      "loss: 1.939599  [14144/16200]\n",
      "loss: 2.027520  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.909064 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.932851  [   64/16200]\n",
      "loss: 1.987130  [ 1344/16200]\n",
      "loss: 1.981726  [ 2624/16200]\n",
      "loss: 1.928765  [ 3904/16200]\n",
      "loss: 1.947140  [ 5184/16200]\n",
      "loss: 1.860206  [ 6464/16200]\n",
      "loss: 1.838504  [ 7744/16200]\n",
      "loss: 1.828386  [ 9024/16200]\n",
      "loss: 1.821025  [10304/16200]\n",
      "loss: 1.928842  [11584/16200]\n",
      "loss: 1.869042  [12864/16200]\n",
      "loss: 1.889805  [14144/16200]\n",
      "loss: 2.060395  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 79.9%, Avg loss: 1.856978 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.803445  [   64/16200]\n",
      "loss: 1.809293  [ 1344/16200]\n",
      "loss: 1.899896  [ 2624/16200]\n",
      "loss: 1.801888  [ 3904/16200]\n",
      "loss: 1.756173  [ 5184/16200]\n",
      "loss: 1.870380  [ 6464/16200]\n",
      "loss: 1.779189  [ 7744/16200]\n",
      "loss: 1.818776  [ 9024/16200]\n",
      "loss: 1.753460  [10304/16200]\n",
      "loss: 1.818177  [11584/16200]\n",
      "loss: 1.709624  [12864/16200]\n",
      "loss: 1.817106  [14144/16200]\n",
      "loss: 1.855465  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 80.2%, Avg loss: 1.778979 \n",
      "\n",
      "Test Results\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 1.762972 \n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# model = resnet50(weights=ResNet50_Weights.SENTINEL2_ALL_MOCO, in_chans=13, num_classes=10).to(device) \n",
    "model = load_model(\"eurosat_resnet50_pretrained_imgflips_epochs5.pth\")\n",
    "dataloader_train, dataloader_val, dataloader_test = load_eurosat_data(custom_transform, batch_size=64)\n",
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, model, lr=1e-3, epochs=5)\n",
    "save_model(model, \"eurosat_resnet50_pretrained_imgflips_epochs10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2603663/2779468065.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Before Training\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.486576 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.146402 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.694296  [   64/16200]\n",
      "loss: 0.675779  [ 1344/16200]\n",
      "loss: 0.709526  [ 2624/16200]\n",
      "loss: 0.757847  [ 3904/16200]\n",
      "loss: 0.592432  [ 5184/16200]\n",
      "loss: 0.907699  [ 6464/16200]\n",
      "loss: 0.629734  [ 7744/16200]\n",
      "loss: 0.977323  [ 9024/16200]\n",
      "loss: 0.597705  [10304/16200]\n",
      "loss: 0.695707  [11584/16200]\n",
      "loss: 0.632978  [12864/16200]\n",
      "loss: 0.720215  [14144/16200]\n",
      "loss: 0.732632  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.958194 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.558906  [   64/16200]\n",
      "loss: 0.663115  [ 1344/16200]\n",
      "loss: 0.689020  [ 2624/16200]\n",
      "loss: 0.614705  [ 3904/16200]\n",
      "loss: 0.642183  [ 5184/16200]\n",
      "loss: 0.618881  [ 6464/16200]\n",
      "loss: 0.681600  [ 7744/16200]\n",
      "loss: 0.660640  [ 9024/16200]\n",
      "loss: 0.524645  [10304/16200]\n",
      "loss: 0.679727  [11584/16200]\n",
      "loss: 0.779067  [12864/16200]\n",
      "loss: 0.739911  [14144/16200]\n",
      "loss: 0.749111  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.799182 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.568150  [   64/16200]\n",
      "loss: 0.739629  [ 1344/16200]\n",
      "loss: 0.590552  [ 2624/16200]\n",
      "loss: 0.583263  [ 3904/16200]\n",
      "loss: 0.585090  [ 5184/16200]\n",
      "loss: 0.523958  [ 6464/16200]\n",
      "loss: 0.578763  [ 7744/16200]\n",
      "loss: 0.570190  [ 9024/16200]\n",
      "loss: 0.526370  [10304/16200]\n",
      "loss: 0.484754  [11584/16200]\n",
      "loss: 0.453992  [12864/16200]\n",
      "loss: 0.482332  [14144/16200]\n",
      "loss: 0.542284  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.867858 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.685017  [   64/16200]\n",
      "loss: 0.600274  [ 1344/16200]\n",
      "loss: 0.545454  [ 2624/16200]\n",
      "loss: 0.532661  [ 3904/16200]\n",
      "loss: 0.581622  [ 5184/16200]\n",
      "loss: 0.564926  [ 6464/16200]\n",
      "loss: 0.621318  [ 7744/16200]\n",
      "loss: 0.618157  [ 9024/16200]\n",
      "loss: 0.516629  [10304/16200]\n",
      "loss: 0.587152  [11584/16200]\n",
      "loss: 0.523001  [12864/16200]\n",
      "loss: 0.837967  [14144/16200]\n",
      "loss: 0.499800  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 38.5%, Avg loss: 3.268022 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.570816  [   64/16200]\n",
      "loss: 0.578171  [ 1344/16200]\n",
      "loss: 0.489441  [ 2624/16200]\n",
      "loss: 0.515378  [ 3904/16200]\n",
      "loss: 0.534994  [ 5184/16200]\n",
      "loss: 0.583912  [ 6464/16200]\n",
      "loss: 0.528600  [ 7744/16200]\n",
      "loss: 0.614623  [ 9024/16200]\n",
      "loss: 0.643625  [10304/16200]\n",
      "loss: 0.499024  [11584/16200]\n",
      "loss: 0.578778  [12864/16200]\n",
      "loss: 0.591858  [14144/16200]\n",
      "loss: 0.478919  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 47.3%, Avg loss: 2.457773 \n",
      "\n",
      "Test Results\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 3.710660 \n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# model = resnet50(weights=None, in_chans=13, num_classes=10).to(device)\n",
    "model = load_model(\"eurosatspatial_resnet50_imgflips_epochs5.pth\")\n",
    "dataloader_train, dataloader_val, dataloader_test = load_eurosat_spatial_data(custom_transform, batch_size=64)\n",
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, model, lr=1e-3, epochs=5)\n",
    "save_model(model, \"eurosatspatial_resnet50_imgflips_epochs10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2603663/2779468065.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Before Training\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.485485 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.146341 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.910224  [   64/16200]\n",
      "loss: 0.736308  [ 1344/16200]\n",
      "loss: 0.660054  [ 2624/16200]\n",
      "loss: 0.747586  [ 3904/16200]\n",
      "loss: 0.629617  [ 5184/16200]\n",
      "loss: 0.609183  [ 6464/16200]\n",
      "loss: 0.656615  [ 7744/16200]\n",
      "loss: 0.633227  [ 9024/16200]\n",
      "loss: 0.803281  [10304/16200]\n",
      "loss: 0.568209  [11584/16200]\n",
      "loss: 0.685856  [12864/16200]\n",
      "loss: 0.580444  [14144/16200]\n",
      "loss: 0.822633  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.784715 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.688224  [   64/16200]\n",
      "loss: 0.733352  [ 1344/16200]\n",
      "loss: 0.718779  [ 2624/16200]\n",
      "loss: 0.679840  [ 3904/16200]\n",
      "loss: 0.613732  [ 5184/16200]\n",
      "loss: 0.651815  [ 6464/16200]\n",
      "loss: 0.559885  [ 7744/16200]\n",
      "loss: 0.508569  [ 9024/16200]\n",
      "loss: 0.535761  [10304/16200]\n",
      "loss: 0.556855  [11584/16200]\n",
      "loss: 0.772103  [12864/16200]\n",
      "loss: 0.795359  [14144/16200]\n",
      "loss: 0.577809  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.777745 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.617826  [   64/16200]\n",
      "loss: 0.550010  [ 1344/16200]\n",
      "loss: 0.595806  [ 2624/16200]\n",
      "loss: 0.554636  [ 3904/16200]\n",
      "loss: 0.577910  [ 5184/16200]\n",
      "loss: 0.638088  [ 6464/16200]\n",
      "loss: 0.657946  [ 7744/16200]\n",
      "loss: 0.474689  [ 9024/16200]\n",
      "loss: 0.629258  [10304/16200]\n",
      "loss: 0.699602  [11584/16200]\n",
      "loss: 0.636322  [12864/16200]\n",
      "loss: 0.637252  [14144/16200]\n",
      "loss: 0.488390  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.807419 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.761545  [   64/16200]\n",
      "loss: 0.662930  [ 1344/16200]\n",
      "loss: 0.607373  [ 2624/16200]\n",
      "loss: 0.665654  [ 3904/16200]\n",
      "loss: 0.611072  [ 5184/16200]\n",
      "loss: 0.479754  [ 6464/16200]\n",
      "loss: 0.419082  [ 7744/16200]\n",
      "loss: 0.503854  [ 9024/16200]\n",
      "loss: 0.503432  [10304/16200]\n",
      "loss: 0.561903  [11584/16200]\n",
      "loss: 0.481088  [12864/16200]\n",
      "loss: 0.525559  [14144/16200]\n",
      "loss: 0.599081  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.746269 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.630612  [   64/16200]\n",
      "loss: 0.591606  [ 1344/16200]\n",
      "loss: 0.617645  [ 2624/16200]\n",
      "loss: 0.536638  [ 3904/16200]\n",
      "loss: 0.679662  [ 5184/16200]\n",
      "loss: 0.676759  [ 6464/16200]\n",
      "loss: 0.431500  [ 7744/16200]\n",
      "loss: 0.453996  [ 9024/16200]\n",
      "loss: 0.551014  [10304/16200]\n",
      "loss: 0.654486  [11584/16200]\n",
      "loss: 0.547094  [12864/16200]\n",
      "loss: 0.370319  [14144/16200]\n",
      "loss: 0.669662  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.754472 \n",
      "\n",
      "Test Results\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.906227 \n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# model = resnet50(weights=ResNet50_Weights.SENTINEL2_ALL_MOCO, in_chans=13, num_classes=10).to(device) \n",
    "model = load_model(\"eurosatspatial_resnet50_imgflips_epochs5.pth\")\n",
    "dataloader_train, dataloader_val, dataloader_test = load_eurosat_spatial_data(custom_transform, batch_size=64)\n",
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, model, lr=1e-3, epochs=5)\n",
    "save_model(model, \"eurosatspatial_resnet50_pretrained_imgflips_epochs10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try 20 Epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2617155/2779468065.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Before Training\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 1.187338 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 78.7%, Avg loss: 1.195844 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.228060  [   64/16200]\n",
      "loss: 1.183465  [ 1344/16200]\n",
      "loss: 1.162653  [ 2624/16200]\n",
      "loss: 1.152022  [ 3904/16200]\n",
      "loss: 1.094136  [ 5184/16200]\n",
      "loss: 1.092738  [ 6464/16200]\n",
      "loss: 1.157012  [ 7744/16200]\n",
      "loss: 1.193702  [ 9024/16200]\n",
      "loss: 1.300223  [10304/16200]\n",
      "loss: 1.100065  [11584/16200]\n",
      "loss: 1.094736  [12864/16200]\n",
      "loss: 1.070426  [14144/16200]\n",
      "loss: 1.009488  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 79.9%, Avg loss: 1.052485 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.039404  [   64/16200]\n",
      "loss: 1.021817  [ 1344/16200]\n",
      "loss: 1.087801  [ 2624/16200]\n",
      "loss: 1.126137  [ 3904/16200]\n",
      "loss: 1.035544  [ 5184/16200]\n",
      "loss: 1.073685  [ 6464/16200]\n",
      "loss: 1.156154  [ 7744/16200]\n",
      "loss: 1.178079  [ 9024/16200]\n",
      "loss: 0.954359  [10304/16200]\n",
      "loss: 1.011315  [11584/16200]\n",
      "loss: 1.014458  [12864/16200]\n",
      "loss: 1.095398  [14144/16200]\n",
      "loss: 0.923170  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.934927 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.043041  [   64/16200]\n",
      "loss: 0.944042  [ 1344/16200]\n",
      "loss: 1.005823  [ 2624/16200]\n",
      "loss: 1.111743  [ 3904/16200]\n",
      "loss: 0.879093  [ 5184/16200]\n",
      "loss: 1.277514  [ 6464/16200]\n",
      "loss: 1.063925  [ 7744/16200]\n",
      "loss: 0.687683  [ 9024/16200]\n",
      "loss: 0.980528  [10304/16200]\n",
      "loss: 0.979607  [11584/16200]\n",
      "loss: 0.984010  [12864/16200]\n",
      "loss: 0.859567  [14144/16200]\n",
      "loss: 0.997111  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.842309 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.069071  [   64/16200]\n",
      "loss: 1.045907  [ 1344/16200]\n",
      "loss: 0.890917  [ 2624/16200]\n",
      "loss: 0.954650  [ 3904/16200]\n",
      "loss: 1.019150  [ 5184/16200]\n",
      "loss: 0.907509  [ 6464/16200]\n",
      "loss: 0.840840  [ 7744/16200]\n",
      "loss: 0.798811  [ 9024/16200]\n",
      "loss: 0.808505  [10304/16200]\n",
      "loss: 0.910563  [11584/16200]\n",
      "loss: 0.739464  [12864/16200]\n",
      "loss: 0.858687  [14144/16200]\n",
      "loss: 0.925922  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.763328 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.846075  [   64/16200]\n",
      "loss: 0.723456  [ 1344/16200]\n",
      "loss: 0.731013  [ 2624/16200]\n",
      "loss: 0.734102  [ 3904/16200]\n",
      "loss: 0.762258  [ 5184/16200]\n",
      "loss: 0.773053  [ 6464/16200]\n",
      "loss: 0.679487  [ 7744/16200]\n",
      "loss: 0.591668  [ 9024/16200]\n",
      "loss: 0.762932  [10304/16200]\n",
      "loss: 0.690452  [11584/16200]\n",
      "loss: 0.772082  [12864/16200]\n",
      "loss: 0.786150  [14144/16200]\n",
      "loss: 0.773259  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.689458 \n",
      "\n",
      "Test Results\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.688902 \n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# model = resnet50(weights=ResNet50_Weights.SENTINEL2_ALL_MOCO, in_chans=13, num_classes=10).to(device) \n",
    "model = load_model(\"eurosat_resnet50_pretrained_imgflips_epochs20.pth\")\n",
    "dataloader_train, dataloader_val, dataloader_test = load_eurosat_data(custom_transform, batch_size=64)\n",
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, model, lr=1e-3, epochs=10)\n",
    "save_model(model, \"eurosat_resnet50_pretrained_imgflips_epochs20.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2617155/2779468065.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Before Training\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 3.701425 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 47.1%, Avg loss: 2.451574 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.574074  [   64/16200]\n",
      "loss: 0.629777  [ 1344/16200]\n",
      "loss: 0.654930  [ 2624/16200]\n",
      "loss: 0.540725  [ 3904/16200]\n",
      "loss: 0.490116  [ 5184/16200]\n",
      "loss: 0.594053  [ 6464/16200]\n",
      "loss: 0.379453  [ 7744/16200]\n",
      "loss: 0.547520  [ 9024/16200]\n",
      "loss: 0.694209  [10304/16200]\n",
      "loss: 0.403760  [11584/16200]\n",
      "loss: 0.542404  [12864/16200]\n",
      "loss: 0.636647  [14144/16200]\n",
      "loss: 0.504249  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.086116 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.541572  [   64/16200]\n",
      "loss: 0.540981  [ 1344/16200]\n",
      "loss: 0.760079  [ 2624/16200]\n",
      "loss: 0.339252  [ 3904/16200]\n",
      "loss: 0.560291  [ 5184/16200]\n",
      "loss: 0.626151  [ 6464/16200]\n",
      "loss: 0.663165  [ 7744/16200]\n",
      "loss: 0.488526  [ 9024/16200]\n",
      "loss: 0.485603  [10304/16200]\n",
      "loss: 0.611368  [11584/16200]\n",
      "loss: 0.355815  [12864/16200]\n",
      "loss: 0.450945  [14144/16200]\n",
      "loss: 0.542541  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.875216 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.489480  [   64/16200]\n",
      "loss: 0.481629  [ 1344/16200]\n",
      "loss: 0.535695  [ 2624/16200]\n",
      "loss: 0.650734  [ 3904/16200]\n",
      "loss: 0.382740  [ 5184/16200]\n",
      "loss: 0.498400  [ 6464/16200]\n",
      "loss: 0.349686  [ 7744/16200]\n",
      "loss: 0.450361  [ 9024/16200]\n",
      "loss: 0.438901  [10304/16200]\n",
      "loss: 0.381232  [11584/16200]\n",
      "loss: 0.536268  [12864/16200]\n",
      "loss: 0.501944  [14144/16200]\n",
      "loss: 0.470201  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.709307 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.437616  [   64/16200]\n",
      "loss: 0.485004  [ 1344/16200]\n",
      "loss: 0.463512  [ 2624/16200]\n",
      "loss: 0.663541  [ 3904/16200]\n",
      "loss: 0.447984  [ 5184/16200]\n",
      "loss: 0.609585  [ 6464/16200]\n",
      "loss: 0.496836  [ 7744/16200]\n",
      "loss: 0.502402  [ 9024/16200]\n",
      "loss: 0.378234  [10304/16200]\n",
      "loss: 0.407320  [11584/16200]\n",
      "loss: 0.420497  [12864/16200]\n",
      "loss: 0.617629  [14144/16200]\n",
      "loss: 0.433088  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 47.7%, Avg loss: 2.740280 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.665883  [   64/16200]\n",
      "loss: 0.416876  [ 1344/16200]\n",
      "loss: 0.417435  [ 2624/16200]\n",
      "loss: 0.354149  [ 3904/16200]\n",
      "loss: 0.443590  [ 5184/16200]\n",
      "loss: 0.536241  [ 6464/16200]\n",
      "loss: 0.449767  [ 7744/16200]\n",
      "loss: 0.465602  [ 9024/16200]\n",
      "loss: 0.427403  [10304/16200]\n",
      "loss: 0.541759  [11584/16200]\n",
      "loss: 0.565924  [12864/16200]\n",
      "loss: 0.745323  [14144/16200]\n",
      "loss: 0.326766  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 51.3%, Avg loss: 2.749391 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.348580  [   64/16200]\n",
      "loss: 0.383016  [ 1344/16200]\n",
      "loss: 0.528018  [ 2624/16200]\n",
      "loss: 0.316423  [ 3904/16200]\n",
      "loss: 0.409040  [ 5184/16200]\n",
      "loss: 0.427128  [ 6464/16200]\n",
      "loss: 0.446907  [ 7744/16200]\n",
      "loss: 0.403715  [ 9024/16200]\n",
      "loss: 0.430792  [10304/16200]\n",
      "loss: 0.391621  [11584/16200]\n",
      "loss: 0.363246  [12864/16200]\n",
      "loss: 0.307076  [14144/16200]\n",
      "loss: 0.355332  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.674897 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.448763  [   64/16200]\n",
      "loss: 0.431523  [ 1344/16200]\n",
      "loss: 0.513956  [ 2624/16200]\n",
      "loss: 0.456872  [ 3904/16200]\n",
      "loss: 0.431186  [ 5184/16200]\n",
      "loss: 0.499184  [ 6464/16200]\n",
      "loss: 0.359616  [ 7744/16200]\n",
      "loss: 0.449940  [ 9024/16200]\n",
      "loss: 0.390660  [10304/16200]\n",
      "loss: 0.547730  [11584/16200]\n",
      "loss: 0.381683  [12864/16200]\n",
      "loss: 0.407624  [14144/16200]\n",
      "loss: 0.395545  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.557198 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.440444  [   64/16200]\n",
      "loss: 0.546385  [ 1344/16200]\n",
      "loss: 0.474423  [ 2624/16200]\n",
      "loss: 0.388205  [ 3904/16200]\n",
      "loss: 0.385021  [ 5184/16200]\n",
      "loss: 0.596585  [ 6464/16200]\n",
      "loss: 0.342216  [ 7744/16200]\n",
      "loss: 0.424318  [ 9024/16200]\n",
      "loss: 0.284430  [10304/16200]\n",
      "loss: 0.396273  [11584/16200]\n",
      "loss: 0.594313  [12864/16200]\n",
      "loss: 0.449562  [14144/16200]\n",
      "loss: 0.349112  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.804880 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.496116  [   64/16200]\n",
      "loss: 0.334988  [ 1344/16200]\n",
      "loss: 0.271850  [ 2624/16200]\n",
      "loss: 0.428931  [ 3904/16200]\n",
      "loss: 0.375764  [ 5184/16200]\n",
      "loss: 0.358451  [ 6464/16200]\n",
      "loss: 0.401937  [ 7744/16200]\n",
      "loss: 0.466210  [ 9024/16200]\n",
      "loss: 0.371438  [10304/16200]\n",
      "loss: 0.482537  [11584/16200]\n",
      "loss: 0.401521  [12864/16200]\n",
      "loss: 0.428267  [14144/16200]\n",
      "loss: 0.366277  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 53.8%, Avg loss: 1.752182 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.353045  [   64/16200]\n",
      "loss: 0.417886  [ 1344/16200]\n",
      "loss: 0.383105  [ 2624/16200]\n",
      "loss: 0.436588  [ 3904/16200]\n",
      "loss: 0.464259  [ 5184/16200]\n",
      "loss: 0.594503  [ 6464/16200]\n",
      "loss: 0.501369  [ 7744/16200]\n",
      "loss: 0.256919  [ 9024/16200]\n",
      "loss: 0.338992  [10304/16200]\n",
      "loss: 0.446247  [11584/16200]\n",
      "loss: 0.328355  [12864/16200]\n",
      "loss: 0.400066  [14144/16200]\n",
      "loss: 0.461807  [15424/16200]\n",
      "Validation Error: \n",
      " Accuracy: 55.2%, Avg loss: 1.858686 \n",
      "\n",
      "Test Results\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.565576 \n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# model = resnet50(weights=ResNet50_Weights.SENTINEL2_ALL_MOCO, in_chans=13, num_classes=10).to(device) \n",
    "model = load_model(\"eurosatspatial_resnet50_imgflips_epochs10.pth\")\n",
    "dataloader_train, dataloader_val, dataloader_test = load_eurosat_spatial_data(custom_transform, batch_size=64)\n",
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, model, lr=1e-3, epochs=10)\n",
    "save_model(model, \"eurosatspatial_resnet50_pretrained_imgflips_epochs20.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "|   Dataset     |   Split   |   Img Tx          |   Model               |   Epochs  |   Accuracy    |\n",
    "|---------------|-----------|-------------------|-----------------------|-----------|---------------|\n",
    "|   EuroSAT     |   Random  |   Resize          |   ResNet50            |5          | 74.8%         |\n",
    "|   EuroSAT     |   Random  |   Resize          |   ResNet50            |10         | 77.6%         |\n",
    "|   EuroSAT     |   Random  |   Resize          |   ResNet50 Pretrained |5          | 67.6%         |\n",
    "|   EuroSAT     |   Random  |   Resize          |   ResNet50 Pretrained |10         | 74.4%         |\n",
    "|   EuroSAT     |   Spatial |   Resize          |   ResNet50            |5          | 70.5%         |\n",
    "|   EuroSAT     |   Spatial |   Resize          |   ResNet50            |10         | 71.0%         |\n",
    "|   EuroSAT     |   Spatial |   Resize          |   ResNet50 Pretrained |5          | 78.1%         |\n",
    "|   EuroSAT     |   Spatial |   Resize          |   ResNet50 Pretrained |10         | 81.5%         |\n",
    "|   EuroSAT     |   Random  |  Resize & Flip VH |   ResNet50            |5          | 76.2%         |\n",
    "|   EuroSAT     |   Random  |  Resize & Flip VH |   ResNet50            |10         | 71.8%         |\n",
    "|   EuroSAT     |   Random  |  Resize & Flip VH |   ResNet50 Pretrained |5          | 80.4%         |\n",
    "|   EuroSAT     |   Random  |  Resize & Flip VH |   ResNet50 Pretrained |10         | 80.7%         |\n",
    "|   EuroSAT     |   Random  |  Resize & Flip VH |   ResNet50 Pretrained |20         | 86.7%         |\n",
    "|   EuroSAT     |   Spatial |  Resize & Flip VH |   ResNet50            |5          | 50.6%         |\n",
    "|   EuroSAT     |   Spatial |  Resize & Flip VH |   ResNet50            |10         | 30.4%         |\n",
    "|   EuroSAT     |   Spatial |  Resize & Flip VH |   ResNet50 Pretrained |5          | 65.2%         |\n",
    "|   EuroSAT     |   Spatial |  Resize & Flip VH |   ResNet50 Pretrained |10         | 72.5%         |\n",
    "|   EuroSAT     |   Spatial |  Resize & Flip VH |   ResNet50 Pretrained |20         | 38.7%         |\n",
    "\n",
    "<!-- |   EuroSAT     |   Random  |   ResNet50            |0          | 0.00%         | -->\n",
    "<!-- |   EuroSAT     |   Random  |   ResNet50 Pretrained |0          | 10.0%         | -->\n",
    "<!-- |   EuroSAT     |   Spatial |   ResNet50            |0          | 0.00%         | -->\n",
    "<!-- |   EuroSAT     |   Spatial |   ResNet50 Pretrained |0          | 0.00%         | -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "#### To Do's\n",
    "1. ~~Experiment with the batch size and learning rate~~\n",
    "1. ~~Figure out how to save down the model parameters after training~~\n",
    "1. Figure out how to implement early stopping based on validation loss\n",
    "1. ~~Consider running the model for 50 epochs~~\n",
    "1. Plot train and val loss over time for given model runs\n",
    "1. Add a dynamic learning rate scheduler\n",
    "1. Try DiNo weights\n",
    "\n",
    "#### Uses of LLMs\n",
    "1. Code completion suggestions while coding in VS Code\n",
    "1. Get through a bug when using ResNet 50, I was getting an error that the dataloader was not iterable. Turns out it's because of the difference between TorchGeo datasets and other pytorch datasets and the way that they store the data and labels as a dictionary. Co-pilot helped me rewrite the training function to pull out the data and labels from the dataloader\n",
    "1. After this bug was fixed, I hit another error that my data was the wrong size for the ResNet. I figured that this was because the EuroSat images were 64x64 while ResNet50 was expecting 224x224 sized images. I wasn't sure which library to use for the transform, and Co-pilot helped me define a custom transform that could handle the TorchGeo dataset as a dictionary to leave my downstream code intact. \n",
    "1. Copilot helped me write the code to modify the ResNet50 to predict 10 classes instead of 1000. \n",
    "1. I got a runtime error that the input type and the weight type should be the same (torch.FloatTensor torch.cuda.FloatTensor). I wasn't sure what this meant so I ran it through Co-pilot which suggested that perhaps it's because my data and model were not both on the GPU, so I added code to the train / test script to move the data onto the device and that worked.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
